{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times when modeling we are asked the question -- How does the model work?  Why should we trust this model?  My response as a data scientist is usually \"because we have rigorously proved model performance on a holdout testset with splits that are realistic to the real world\". Oftentimes that is not enough to convince domain experts.\n",
    "\n",
    "[LIME](https://homes.cs.washington.edu/~marcotcr/blog/lime/) is a tool which can help with this problem.  It uses local perturbations of featurespace to determine feature importance.  \n",
    "\n",
    "![Selection_110.png](assets/lime_dog.png)\n",
    "\n",
    "So if this tool can work in human understandable ways for images can it work on molecules?  In this tutorial I will show how to use LIME for model interpretability for any of our fixed-length featurization models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making of the Model\n",
    "\n",
    "The first thing we have to do is train a model.  Here we are going to train a toxicity model using Circular fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imaging imports to get pictures in the notebook\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from deepchem.molnet import load_tox21\n",
    "\n",
    "# Only for debug!\n",
    "np.random.seed(123)\n",
    "\n",
    "# Load Tox21 dataset\n",
    "n_features = 1024\n",
    "tox21_tasks, tox21_datasets, transformers = load_tox21()\n",
    "train_dataset, valid_dataset, test_dataset = tox21_datasets\n",
    "\n",
    "# Fit models\n",
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "nb_epoch = 10\n",
    "model = dc.models.tensorgraph.fcnet.MultitaskClassifier(\n",
    "    len(tox21_tasks),\n",
    "    train_dataset.get_data_shape()[0])\n",
    "\n",
    "# Fit trained model\n",
    "model.fit(train_dataset, nb_epoch=nb_epoch)\n",
    "model.save()\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_scores = model.evaluate(train_dataset, [metric], transformers)\n",
    "valid_scores = model.evaluate(valid_dataset, [metric], transformers)\n",
    "\n",
    "print(\"Train scores\")\n",
    "print(train_scores)\n",
    "\n",
    "print(\"Validation scores\")\n",
    "print(valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LIME\n",
    "\n",
    "So LIME as is can work on any problem with a fixed size input vector.  It works by computing probability distributions for the individual features and covariance between the features.\n",
    "\n",
    "We are going to create an explainer for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_tabular\n",
    "feature_names = [\"fp_%s\"  % x for x in range(1024)]\n",
    "explainer = lime_tabular.LimeTabularExplainer(train_dataset.X, \n",
    "                                              feature_names=feature_names, \n",
    "                                              categorical_features=feature_names,\n",
    "                                              class_names=['not toxic', 'toxic'], \n",
    "                                              discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to attempt to explain why the model predicts a molecule to be toxic for NR-AR\n",
    "The specific assay details can be found [here](https://pubchem.ncbi.nlm.nih.gov/bioassay/743040)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a function which takes a 2d numpy array (samples, features) and returns predictions (samples,)\n",
    "def eval_model(my_model, transformers):\n",
    "    def eval_closure(x):\n",
    "        ds = dc.data.NumpyDataset(x, None, None, None)\n",
    "        # The 0th task is NR-AR\n",
    "        predictions = model.predict(ds)[:,0]\n",
    "        return predictions\n",
    "    return eval_closure\n",
    "model_fn = eval_model(model, transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to investigate a toxic compound\n",
    "active_id = np.where(test_dataset.y[:,0]==1)[0][0]\n",
    "print(active_id)\n",
    "Chem.MolFromSmiles(test_dataset.ids[active_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns an Lime Explainer class\n",
    "# The explainer contains details for why the model behaved the way it did\n",
    "exp = explainer.explain_instance(test_dataset.X[active_id], model_fn, num_features=5, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are in an ipython notebook it can show it to us\n",
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![explanation output](assets/lime_why_toxic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model believes that fingerprint 118, 519 and 301 highly contribute to the molecule's toxicity.  We can reverse our the hash function and look at the fragments that activated those fingerprints for this molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "def fp_mol(mol, fp_length=1024):\n",
    "    \"\"\"\n",
    "    returns: dict of <int:list of string>\n",
    "        dictionary mapping fingerprint index\n",
    "        to list of smile string that activated that fingerprint\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    feat = dc.feat.CircularFingerprint(sparse=True, smiles=True, size=1024)\n",
    "    retval = feat._featurize(mol)\n",
    "    for k, v in retval.items():\n",
    "        index = k % 1024\n",
    "        if index not in d:\n",
    "            d[index] = set()\n",
    "        d[index].add(v['smiles'])\n",
    "    return d\n",
    "# What fragments activated what fingerprints in our active molecule?\n",
    "my_fp = fp_mol(Chem.MolFromSmiles(test_dataset.ids[active_id]))\n",
    "\n",
    "# We can calculate which fragments activate all fingerprint\n",
    "# indexes throughout our entire training set\n",
    "all_train_fps = {}\n",
    "X = train_dataset.X\n",
    "ids = train_dataset.ids\n",
    "for i in range(len(X)):\n",
    "    d = fp_mol(Chem.MolFromSmiles(ids[i]))\n",
    "    for k, v in d.items():\n",
    "        if k not in all_train_fps:\n",
    "            all_train_fps[k] = set()\n",
    "        all_train_fps[k].update(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize which fingerprints our model declared toxic for the\n",
    "# active molecule we are investigating\n",
    "Chem.MolFromSmiles(list(my_fp[118])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(list(my_fp[519])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(list(my_fp[301])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also see what fragments are missing by investigating the training set \n",
    "# According to our explanation having one of these fragments would make our molecule more\n",
    "# likely to be toxic\n",
    "Chem.MolFromSmiles(list(all_train_fps[381])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(list(all_train_fps[381])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(list(all_train_fps[381])[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(list(all_train_fps[875])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(list(all_train_fps[875])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(list(all_train_fps[875])[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LIME on fragment based models can give you intuition over which fragments are contributing to your response variable in a linear fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
